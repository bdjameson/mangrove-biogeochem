---
title: "Microelectrode Data Processing Workflow"
author: "Brett D. Jameson"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: flatly
    code_folding: show
    df_print: paged
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 3
    latex_engine: xelatex
editor_options:
  chunk_output_type: console
fontsize: 11pt
geometry: margin=1in
---

# üåä Overview

This notebook implements a **reproducible workflow** for correcting **sensor signal drift** in microelectrode (microprofile) measurements collected from sediment environments. Data were collected as part of the 2024 Mangrove Biogeochemistry project. Freshly collected mangrove sediment cores were incubated daily under varying nutrient amendment regimes and profiled to characterize vertical distributions of N2O, NO, H2S, and O2. Porewater profiles were obtained following light and dark incubations using Clark-type microelectrodes from Unisense. Temperatures were monitored throughout each experiment using HOBO temp/light pendant loggers.

üß≠ **Workflow outline:**

1. Import and trim HOBO temperature logs  
2. Combine temperature, profile, and calibration datasets  
3. Apply drift corrections  
4. Compute final analyte concentrations using calibration curves 
5. Adjust profiles to standardized baselines  
6. Generate exploratory data plots
7. Export corrected profiles for further interpretation 

Each section includes brief narrative guidance followed by executable R code.

# ‚öôÔ∏è Packages and Global Options

Load core packages for data manipulation, modeling, and visualization.

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(purrr)
library(broom)
library(fuzzyjoin)
library(car)
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)


knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE,
fig.width = 8,
fig.height = 6
)
```


# üóÇÔ∏è Metadata and HOBO Logger Data

## üßæ Import Metadata

This section imports experimental metadata (incubation intervals, flume assignments, light conditions) and parses date formats. 

```{r read_metadata, message=FALSE, warning=FALSE}
dir_compiled <- "./compiled-data"
meta_path <- file.path(dir_compiled, "MBGC.experiment.metadata.csv")

# Helper: parse "m/d/yy HH:MM" else fall back to "m/d/yy"
parse_mdy_hm_or_date <- function(x) {
  x <- trimws(as.character(x))
  x[x %in% c("", "NA", "N/A", "na", "n/a")] <- NA_character_
  # Try datetime first
  dt <- suppressWarnings(lubridate::mdy_hm(x, tz = "UTC"))
  need_date_only <- is.na(dt) & !is.na(x)
  if (any(need_date_only)) {
    dt[need_date_only] <- as.POSIXct(
      lubridate::mdy(x[need_date_only], tz = "UTC"),
      tz = "UTC"
    )
  }
  dt
}

metadata <- readr::read_csv(
  meta_path,
  # Force everything to character so we control parsing deterministically
  col_types = readr::cols(.default = readr::col_character()),
  trim_ws = TRUE,
  show_col_types = FALSE
) |>
  janitor::clean_names() |>
  mutate(
    experiment      = as.character(experiment),
    amendment       = as.character(amendment),
    collection_date = lubridate::mdy(collection_date, tz = "UTC"),              # Date only ‚Üí Date class is fine
    start_date      = parse_mdy_hm_or_date(start_date),                          # POSIXct
    end_date        = parse_mdy_hm_or_date(end_date),                            # POSIXct
    low_tide        = parse_mdy_hm_or_date(low_tide),                            # POSIXct
    cores_in        = parse_mdy_hm_or_date(cores_in),                            # POSIXct
    nuts_in         = parse_mdy_hm_or_date(nuts_in),                             # POSIXct
    dark_start      = parse_mdy_hm_or_date(dark_start),                          # POSIXct
    light_start     = parse_mdy_hm_or_date(light_start)                          # POSIXct
  )

# Diagnostics: confirm types are correct
print(sapply(metadata, class))

# Optional: quick peek
# glimpse(metadata)
# metadata |> select(experiment, start_date, end_date, dark_start, light_start) |> print(n = Inf)
```

## üå°Ô∏è Import and Trim HOBO Logs. 

Incubation temperatures were recorded using HOBO pendant temperature loggers. Here each HOBO temperature log is read and trimmed to the incubation period defined in the metadata file.

### üß© Map HOBO data to metadata table

```{r hobo_read, message=FALSE, warning=FALSE}
# Path to HOBO CSVs
path_hobo <- "./hobo-data/"

# List CSV files
file_names <- list.files(
  path = path_hobo,
  pattern = "\\.csv$",
  full.names = FALSE
)

# Optional: omit metadata files
# file_names <- file_names[!grepl("metadata", file_names, ignore.case = TRUE)]

# Desired HOBO column names (match your logger export)
hobo_colnames <- c(
  "obs", "date_time", "tempC",
  "coupler_attached", "host_connected",
  "stopped", "end_of_file"
)

# Timestamp format in files
fmt_hobo <- "%m/%d/%y %I:%M:%S %p"

# Build table, parse experiment/flume by splitting on "_"
df <- tibble(file_name = file_names) |>
  mutate(
    # remove extension then split on underscores
    base = tools::file_path_sans_ext(file_name)
  ) |>
  tidyr::separate(
    base,
    into = paste0("p", 1:12),
    sep = "_",
    fill = "right",
    remove = FALSE
  ) |>
  mutate(
    # replicate your original regex intent:
    # experiment = first two tokens joined by "_"
    experiment = paste(p1, p2, sep = "_"),
    # flume = 5th token (based on your original gsub pattern)
    flume = p5
  ) |>
  select(-starts_with("p"), -base) |>
  mutate(
    # attach metadata rows for each experiment
    info = purrr::map(experiment, ~ dplyr::filter(metadata, experiment == .x)),
    # read each CSV with consistent colnames; skip header lines
    temp_raw = purrr::map(
      file_name,
      ~ readr::read_csv(
        file.path(path_hobo, .x),
        col_names = hobo_colnames,
        skip = 2,
        show_col_types = FALSE,
        progress = FALSE
      )
    ),
    # parse timestamps inside each nested df
    temp_data = purrr::map(
      temp_raw,
      ~ dplyr::mutate(.x, date_time = as.POSIXct(date_time, format = fmt_hobo, tz = "UTC"))
    )
  ) |>
  select(-temp_raw)

# Optional quick diagnostic
df |>
  transmute(
    experiment,
    flume,
    start_time = purrr::map_chr(temp_data, ~ format(min(.x$date_time, na.rm = TRUE), "%Y-%m-%d %H:%M")),
    end_time   = purrr::map_chr(temp_data, ~ format(max(.x$date_time, na.rm = TRUE), "%Y-%m-%d %H:%M")),
    n_obs      = purrr::map_int(temp_data, nrow)
  ) |>
  print(n = Inf)
```

### ‚úÇÔ∏è Trim HOBO Logger Data to Incubation Intervals

For each experiment, HOBO temperature records are filtered so that only timestamps within the incubation period are retained.
This removes pre- or post-incubation readings that are not relevant to the analysis.

```{r hobo_trim, message=FALSE, warning=FALSE}
df_trimmed <- df |>
  mutate(
    temp_trimmed = purrr::map2(
      temp_data, info,
      ~ dplyr::filter(
        .x,
        date_time >= .y$start_date,
        date_time <= .y$end_date
      )
    )
  )
```

Next, combine data files by experiment and add 'flume' identifier for temperature log plotting.

```{r hobo_combine, message=FALSE, warning=FALSE}
# Combine HOBO data by experiment and add flume identifier for plotting
df_trimmed <- df_trimmed %>%
  select(-file_name, -temp_data) %>%       
  group_by(experiment) %>%
  nest(temp = c(flume, temp_trimmed)) %>%  
  mutate(
    temp = map(temp, function(df) {
      # Unnest the 'temp' dataframe while keeping 'flume' intact
      df %>% 
        unnest_longer(temp_trimmed) %>%    
        unpack(temp_trimmed)
    })
  )
```

üìà HOBO Exploratory Plots

Before proceeding with calibration and correction, it‚Äôs important to visually confirm that the **temperature records** align with the defined incubation periods.  
This step helps identify any sensor irregularities or logger drift prior to use in later modeling steps.

The code below generates exploratory plots for each experiment, displaying temperature variation over time across flumes.  
Vertical reference lines mark key events:
- üü¶ **Dark incubation start**
- üü© **Light incubation start**
- üü• **End of incubation**

```{r hobo_plot, message=FALSE, warning=FALSE}
df_trimmed <- df_trimmed |>
  mutate(
    temp_plot = pmap(
      list(.x = temp, .y = info, .z = experiment),
      function(.x, .y, .z) {
        ggplot(.x, aes(x = as.POSIXct(date_time), y = tempC, color = flume)) +
          geom_vline(xintercept = .y$dark_start, color = "darkblue") +
          geom_vline(xintercept = .y$light_start, color = "darkgreen") +
          geom_vline(xintercept = .y$end_date, color = "darkred") +
          geom_line() +
          geom_point(size = 1) +
          scale_color_manual(
            values = c("#1B9E77", "#D95F02", "#7570B3", "#E6AB02"),
            name = "Flume"
          ) +
          labs(title = .z, x = "Time (UTC)", y = "Temperature (¬∞C)") +
          theme_bw(base_size = 11) +
          theme(legend.position = "bottom")
      }
    )
  )

df_trimmed |>
  filter(experiment == "Mangrove_8") |>
  pull(temp_plot)
```

# üß™ Profiles and Calibrations

This section imports and organizes **microelectrode profile data** and their corresponding **sensor calibrations**.  
Microprofiles for O‚ÇÇ, N‚ÇÇO, NO, and H‚ÇÇS were collected using Clark-type microelectrodes (Unisense) during both light and dark incubations.  
Each dataset is cleaned, formatted, and nested by experiment to enable downstream signal correction and calibration mapping.

## üì• Read Microprofile Data

Microprofiles (O‚ÇÇ, N‚ÇÇO, NO, H‚ÇÇS) are imported and structured into **nested data frames** for each experiment.  
Datetime values are standardized to `POSIXct` and categorical variables (e.g., `flume`, `treatment`, `channel`) are converted to factors for consistency.

```{r profile_read, message=FALSE, warning=FALSE}
# Read microprofile data
profiles <- readr::read_csv("./compiled-data/MBGC.mangrove.microprofiles.2024.csv") |>
  dplyr::rename(date_time = time) |>
  dplyr::mutate(
    date_time  = as.POSIXct(date_time, format = "%Y-%m-%d %H:%M", tz = "UTC"),
    date       = as.Date(date, format = "%d/%m/%Y"),
    across(c(flume, core, treatment, replicate, channel), as.factor)
  )
```

## ‚öôÔ∏è Read Calibration Data

Two-point **sensor calibration data** (slope and intercept) are imported for each experiment and formatted for use in subsequent signal correction steps.  

Each calibration record corresponds to a specific **sensor channel** and **analyte**

```{r calibration_input, message=FALSE, warning=FALSE}
cal_trim <- readr::read_csv(
  "./compiled-data/MBGC.sensor.calibrations.2024.csv",
  name_repair = "minimal"     # preserve original column names exactly
) |>
  dplyr::group_by(experiment, analyte, light.dark, cal.ID, channel) |>
  dplyr::distinct(experiment, .keep_all = TRUE) |>
  dplyr::select(-c(cal.point, `Calibration Time Point`, concentration, signal.mV)) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    cal_time = as.POSIXct(time, format = "%Y-%m-%d %H:%M", tz = "UTC"),
    channel  = as.factor(channel)
  ) |>
  dplyr::select(-time)

# ‚úÖ Quick check: confirm structure and units
glimpse(cal_trim)
```

üß© Note:
In this workflow, two-point calibrations were performed before each profiling session.
When multiple calibrations were available for a single experiment (e.g., mid-run adjustments), the workflow automatically selects the most appropriate calibration based on timing and consistency across channels.

# üîó Data Integration

This section combines the **HOBO temperature logs**, **microelectrode profiles**, and **sensor calibration data** into a single, unified data structure.  
The resulting **nested tibble** links each experiment with its corresponding temperature record (`temp`), microprofiles (`profiles`), and calibration coefficients (`cals`), preparing the dataset for subsequent drift correction and calibration application.

## üß© Combine HOBO, Profile, and Calibration Data

For each experiment, the relevant microprofiles and calibrations are attached to the corresponding temperature data.  
This step ensures that all datasets share consistent experiment-level identifiers and temporal alignment before correction and analysis.

```{r profile_combine, message=FALSE, warning=FALSE}
# Combine temperature logs with profiles and calibrations
df_combined <- df_trimmed |>
  dplyr::select(-temp_plot) |>
  dplyr::mutate(
    profiles = purrr::map(
      experiment,
      ~ profiles |>
        dplyr::filter(experiment == .x) |>
        tibble::as_tibble()
    ),
    cals = purrr::map(
      experiment,
      ~ cal_trim |>
        dplyr::filter(experiment == .x) |>
        tibble::as_tibble()
    )
  )
```

# üß≠ Quality Control

Before applying corrections or calibrations, it‚Äôs important to visually inspect the **raw sensor data** for consistency and potential anomalies.

This step helps identify outlier signals, noisy sensors, or incomplete profiles prior to quantitative processing.

## üìä Exploratory QC Plots (Raw Data)

Visualize **raw microprofile voltage (mV)** distributions for each analyte before any quality control filtering or signal correction.  
These plots provide a quick way to confirm that:
- Sensor channels are functioning properly  
- Profile depths are correctly aligned  
- Signal magnitudes fall within expected ranges  

```{r microprofile_qc, message=FALSE, warning=FALSE}
df_combined <- df_combined |>
  mutate(
    profile_plot = purrr::map2(
      profiles, experiment,
      ~ ggplot(.x, aes(x = mV, y = depth, color = core)) +
          geom_path(aes(linetype = channel)) +
          geom_point(aes(shape = channel)) +
          labs(title = .y) +
          scale_y_reverse() +
          facet_wrap(~flume + light.dark + analyte, ncol = 4, scales = "free_x") +
          theme_bw()
    )
  )

# Example: inspect one experiment‚Äôs raw data
df_combined |>
  filter(experiment == "Mangrove_6") |>
  pull(profile_plot)
```

## üßπ Remove Duplicated Profiles

Occasionally, certain cores were profiled **more than once** when the initial attempt failed or produced a noisy signal.  
This step filters the dataset so that only the **best-quality replicate** is retained for each unique combination of incubation, flume, and channel.

```{r qc_duplicates, message=FALSE, warning=FALSE}
df_combined <- df_combined |>
  mutate(
    profiles = purrr::map(
      profiles,
      ~ .x |>
          group_by(date, incubation, experiment, light.dark, treatment, flume, core) |>
          dplyr::filter(if (any(replicate == "2")) replicate == "2" else TRUE) |>
          ungroup()
    )
  )
```

üß© Note:
Selection criteria can be customized ‚Äî for example, retaining the replicate with the highest signal strength, most complete depth coverage, or least noise.

## ‚öóÔ∏è Addressing Sulfide Interference

During Nitric Oxide (NO) profiling (notably in Experiment 6), some profiles exhibited strong sulfide interference, seen as sharp spikes in signal near the bottom of the sediment column.
Because the drift correction algorithm assumes analyte concentrations approach zero near the profile base, these contaminated profiles must be removed before correction.

We handle this automatically by filtering out any profiles where the final mV reading exceeds twice the expected zero-point intercept from the calibration curve.

```{r qc_sulfide, message=FALSE, warning=FALSE}
# ---- Step 1: Create a calibration lookup table ----
cals_lookup <- df_combined |>
  tidyr::unnest(cals, names_sep = "_") |>
  dplyr::filter(cals_cal.ID == 1) |>
  dplyr::select(
    cals_experiment, cals_light.dark, cals_channel,
    cals_analyte, cals_intercept
  ) |>
  dplyr::distinct(
    cals_experiment, cals_light.dark, cals_channel,
    .keep_all = TRUE
  )

# ---- Step 2: Filter each profile dataset using the lookup table ----
df_combined <- df_combined |>
  mutate(
    profiles = purrr::map(profiles, \(df_prof) {
      df_prof |>
        dplyr::left_join(
          cals_lookup,
          by = c(
            "experiment"  = "cals_experiment",
            "light.dark"  = "cals_light.dark",
            "channel"     = "cals_channel"
          )
        ) |>
        group_by(experiment, date, incubation, light.dark,
                 treatment, flume, core, analyte, channel) |>
        dplyr::filter(
          if (first(analyte) == "H2S")
            TRUE
          else
            !(last(mV) > (cals_intercept * 2))
        ) |>
        ungroup() |>
        dplyr::select(-cals_analyte, -cals_intercept)
    })
  )
```

üß™ Interpretation guide:

- ‚úÖ Profiles retained: clean signals approaching zero near the sediment base.

- ‚ö†Ô∏è Profiles removed: likely contaminated by H‚ÇÇS cross-sensitivity.

- üí° Removing these early ensures the drift model assumptions remain valid and improves downstream correction accuracy.

## üìà Exploratory Plots (After QC Filtering)

Now that duplicate and contaminated profiles have been removed, we regenerate diagnostic plots to verify that the **replicate selection** and **sulfide interference filtering** worked as intended.

These plots should display:
- One clean replicate per analyte, channel, and flume  
- Smooth signal decay toward the sediment base (for NO, N‚ÇÇO, O‚ÇÇ)  
- Stable, non-spiking H‚ÇÇS profiles

```{r qc_postplot, message=FALSE, warning=FALSE}
df_combined <- df_combined |>
  mutate(
    profile_plot = purrr::map2(
      profiles, experiment,
      ~ ggplot(.x, aes(x = mV, y = depth, color = core)) +
          geom_path(aes(linetype = channel)) +
          geom_point(aes(shape = channel)) +
          labs(title = paste(.y, "- Post-QC")) +
          scale_y_reverse() +
          facet_wrap(~flume + light.dark + analyte, ncol = 4, scales = "free_x") +
          theme_bw()
    )
  )

# Example: inspect one experiment‚Äôs cleaned data
df_combined |>
  filter(experiment == "Mangrove_6") |>
  pull(profile_plot)
```

# ‚öôÔ∏è Drift Correction

Before converting voltages to concentrations, we must correct for signal drift that can occur during profiling.
Drift arises from gradual shifts in the electrode‚Äôs baseline potential, often due to small changes in temperature, electrode polarization, or electronic instability during measurement.

## üå°Ô∏è Map Incubation Temperatures to Microprofile Observations

To quantify and correct for drift, we first link each microprofile time series with the corresponding HOBO temperature records.
This allows us to assess whether baseline changes are associated with temperature variability.

Next, we extract the bottom three readings from each profile ‚Äî assumed to represent true zero-analyte conditions (following Jameson et al., 2024).

These zero points provide a stable reference for modeling baseline drift over time and temperature.

```{r microprofile_drift, message=FALSE, warning=FALSE}
# ==== 1. Define fuzzy join helper ====
# Joins profile data (mV) and HOBO temp data within ¬±5 minutes for each flume.
fuzzy_join_by_time <- function(df_profiles, df_temp, tolerance = 5) {
  fuzzyjoin::fuzzy_left_join(
    df_profiles, df_temp,
    by = c("date_time" = "date_time", "flume" = "flume"),
    match_fun = list(
      function(x, y) abs(as.numeric(difftime(x, y, units = "mins"))) <= tolerance,
      `==`
    ),
    suffix = c(".prof", ".temp")
  )
}

# ==== 2. Fuzzy-join each profiles‚Äìtemperature pair ====
# Keep only the closest temperature observation for each profile time.
df_combined <- df_combined |>
  select(-profile_plot) |>
  mutate(
    profiles = map2(profiles, temp, \(prof_df, temp_df) {
      
      # Perform fuzzy join (within ¬±5 min)
      joined <- fuzzyjoin::fuzzy_left_join(
        prof_df, temp_df,
        by = c("date_time" = "date_time", "flume" = "flume"),
        match_fun = list(
          function(x, y) abs(as.numeric(difftime(x, y, units = "mins"))) <= 5,
          `==`
        )
      )
      
      # Dynamically detect column names created by fuzzyjoin
      dt_cols   <- grep("^date_time", names(joined), value = TRUE)
      fl_cols   <- grep("^flume", names(joined), value = TRUE)
      temp_cols <- grep("^tempC", names(joined), value = TRUE)
      
      date_prof <- dt_cols[1]
      date_temp <- if (length(dt_cols) > 1) dt_cols[2] else dt_cols[1]
      flume_prof <- fl_cols[1]
      temp_col_name <- tail(temp_cols, 1)
      
      # Calculate time difference manually using base R indexing
      joined$time_diff <- abs(as.numeric(
        difftime(joined[[date_prof]], joined[[date_temp]], units = "mins")
      ))
      
      # Pick the closest temperature record per observation & channel
      joined |>
        group_by(across(any_of(c(date_prof, flume_prof, "channel", "analyte", "core", "replicate")))) |>
        slice_min(time_diff, with_ties = FALSE) |>
        ungroup() |>
        rename_with(~ "date_time", all_of(date_prof)) |>
        rename_with(~ "flume", all_of(flume_prof)) |>
        rename_with(~ "tempC", all_of(temp_col_name)) |>
        select(
          -all_of(date_temp),
          -time_diff,
          tidyselect::any_of(c(
            "coupler.attached", "host.connected",
            "stopped", "end.of.file"
          ))
        )
    })
  )

# ==== 3. Identify zero (baseline) points for drift assessment ====
# Bottom three readings per profile, except H2S where top three are used.
df_sliced <- df_combined |>
  mutate(
    zeros = purrr::map(
      profiles,
      \(df_prof) {
        df_prof |>
          group_by(
            experiment, light.dark, treatment,
            flume, core, replicate, channel, analyte
          ) |>
          dplyr::filter(
            if_else(analyte == "H2S",
                    row_number() <= 3,
                    row_number() > (n() - 3))
          ) |>
          ungroup()
      }
    )
  )
```

üß™ Interpretation guide:

The fuzzy join ensures temperature and microprofile timestamps align within ¬±5 minutes.

Extracted zero regions serve as the baseline reference for the drift correction models (time- or temperature-based).

## üìâ Visualizing Sensor Drift

Before applying any statistical correction, it‚Äôs important to **visualize how the sensor baseline changes** through time and with temperature.  
This diagnostic step helps identify whether drift is primarily **time-dependent**, **temperature-driven**, or minimal altogether.

We‚Äôll generate exploratory plots showing how the **zero-point voltages (mV)** vary across each experiment and analyte.  
These visual checks provide valuable intuition for choosing between the **time-based** and **temperature-dependent** correction models introduced later.

**Purpose**:

- Detect **temporal trends** in the zero readings (linear or non-linear drift)  
- Evaluate **temperature coupling** between sensor response and incubator conditions  
- Identify sensors with **unstable baselines** or erratic behavior  

```{r drift_plot, message=FALSE, warning=FALSE}
df_sliced <- df_sliced |>
  mutate(
    temp_correct = purrr::map2(
      zeros, experiment,
      ~ ggplot(.x, aes(x = tempC, y = mV)) + # Change x variable to assess time effects. 
        geom_point(aes(shape = core, color = flume)) +
        facet_wrap(~channel + light.dark, scales = "free", nrow = 2) +
        labs(title = paste("Drift assessment ‚Äì", .y),
             x = "Temperature (¬∞C)",
             y = "Signal (mV)") +
        theme_bw()
    )
  )

# Example: inspect relationships for a specific experiment
df_sliced |>
  filter(experiment == "Mangrove_8") |>
  pull(temp_correct)
```

## Drift Modelling Procedure:

### Time-Based Drift Correction

When measurement temperatures are stable and the experimental duration is relatively short (typically less than 4 hours), sensor drift can often be modeled as a simple linear function of time. This correction assumes that drift arises from gradual changes in the sensor‚Äôs electrochemical baseline‚Äîsuch as slow reference electrode potential shifts or membrane stabilization‚Äîrather than external temperature fluctuations.

In this step, a linear regression model (mV ~ time) is fitted independently for each analyte and channel using only the zero-domain points (i.e., the bottom few measurements where true analyte concentration is minimal). The model estimates a baseline drift slope (ŒîmV/sec) and tests its statistical significance.

For each experiment:

- ‚úÖ Only slopes with p < 0.05 are retained for correction (adjust for more stringent correction procedure).

- ‚öôÔ∏è The initial calibration intercept (from the pre-run calibration) is used as the reference baseline.

- üö´ Profiles without significant slopes are left uncorrected, ensuring that random variation is not overfitted.

- The fitted slope is then subtracted from the raw signal (mV) to generate drift-corrected voltages (corrected_mV), which represent the adjusted electrochemical response of the sensor over time.

```{r drift_correction, message=FALSE, warning=FALSE}
# ---- Step 1: Fit Time-Based Drift Model ----
# Define baseline drift using a simple linear regression on time for each analyte and channel.
df_model <- df_sliced |>
  mutate(
    drift_model = map2(zeros, cals, \(zeros_df, cal_df) {

      # ---- Normalize ID columns ----
      zeros_df <- zeros_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark"))
      cal_df <- cal_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark"))

      # ---- Extract first calibration intercept ----
      cal_intercept <- cal_df |>
        filter(cal.ID == 1) |>
        distinct(experiment, light_dark, channel, analyte, intercept)

      # ---- Ensure IDs exist ----
      for (nm in c("experiment", "light_dark", "channel", "analyte")) {
        if (!nm %in% names(zeros_df)) zeros_df[[nm]] <- unique(cal_intercept[[nm]])[1]
      }

      # ---- Fit linear drift model ----
      zeros_df |>
        mutate(
          time_numeric = as.numeric(difftime(date_time, min(date_time), units = "secs"))
        ) |>
        left_join(cal_intercept, by = c("experiment", "light_dark", "channel", "analyte")) |>
        group_by(experiment, light_dark, channel, analyte) |>
        nest() |>
        mutate(
          model  = map(data, \(df) lm(mV ~ time_numeric, data = df)),
          tidied = map(model, tidy)
        ) |>
        unnest(tidied) |>
        filter(term == "time_numeric") |>
        summarise(
          experiment        = first(experiment),
          light_dark        = first(light_dark),
          channel           = first(channel),
          analyte           = first(analyte),
          intercept_adj     = first(cal_intercept$intercept),
          slope             = first(estimate),
          p_value           = first(p.value),
          slope_significant = p_value < 0.05,
          .groups = "drop"
        )
    })
  )

# ---- Step 2: Apply Drift Correction ----
df_corrected <- df_combined |>
  ungroup() |>
  mutate(
    profiles = map2(profiles, df_model$drift_model, \(prof_df, drift_df) {
      prof_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark")) |>
        left_join(
          drift_df |> filter(slope_significant),
          by = c("experiment", "light_dark", "channel", "analyte")
        ) |>
        mutate(
          time_numeric = as.numeric(difftime(date_time, min(date_time), units = "secs")),
          corrected_mV = if_else(
            !is.na(slope) & slope_significant,
            mV - (intercept_adj + slope * time_numeric),
            mV
          )
        ) |>
        select(
          -slope, -intercept_adj, -slope_significant, -p_value, -time_numeric
        )
    })
  )
```

### Temperature Effects Correction 

In experiments where temperature variability may influence sensor baselines, both time and temperature effects can be modeled simultaneously. The model suite includes terms for time, time¬≤, and tempC, automatically simplifying if collinearity is detected (using variance-inflation checks).

For each analyte‚Äìchannel group:

- üìä Models are retained only if adjusted R¬≤ ‚â• 0.3 and at least one predictor is significant (p < 0.01).

- üß† If both time and temperature effects are significant, a combined correction is applied; otherwise, only the relevant term is used.

- üîí If neither is significant, the original signal is preserved.

```{r drift_correction_time_temp, message=FALSE, warning=FALSE}
# Helper: safe VIF checker
safe_vif_check <- function(model) {
  out <- tryCatch(car::vif(model), error = function(e) NA)
  if (all(is.na(out))) return(NA)
  out
}

# ==== 1. Fit drift (time + temperature) models ====
p_thresh <- 0.01   # significance cutoff for slope terms

df_model <- df_sliced |>
  mutate(
    drift_model = map2(zeros, cals, \(zeros_df, cal_df) {
      
      # ---- Extract initial calibration intercepts ----
      cal_intercept <- cal_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark")) |>
        filter(cal.ID == 1) |>
        distinct(experiment, light_dark, channel, analyte, intercept)
      
      zeros_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark")) |>
        group_by(experiment, light_dark, channel, analyte) |>
        mutate(time_numeric = as.numeric(difftime(date_time, min(date_time), units = "secs"))) |>
        left_join(cal_intercept, by = c("experiment", "light_dark", "channel", "analyte")) |>
        do({
          # ---- Fit model with time, time¬≤, and tempC ----
          model <- lm(mV ~ time_numeric + I(time_numeric^2) + tempC, data = .)
          vif_values <- tryCatch(car::vif(model), error = function(e) rep(1, 3))
          
          # Simplify if collinearity is high
          if (any(vif_values > 5, na.rm = TRUE)) {
            model <- lm(mV ~ time_numeric + tempC, data = .)
          }
          
          summary_model <- summary(model)
          coefs <- summary_model$coefficients
          
          # ---- Safe extraction helpers ----
          get_coef <- function(term) if (term %in% rownames(coefs)) coefs[term, "Estimate"] else NA_real_
          get_pval <- function(term) if (term %in% rownames(coefs)) coefs[term, "Pr(>|t|)"] else NA_real_
          get_sig  <- function(term) if (term %in% rownames(coefs)) coefs[term, "Pr(>|t|)"] < p_thresh else FALSE
          
          data.frame(
            experiment        = unique(.$experiment),
            light_dark        = unique(.$light_dark),
            channel           = unique(.$channel),
            analyte           = unique(.$analyte),
            intercept          = get_coef("(Intercept)") - unique(.$intercept),
            time_slope         = get_coef("time_numeric"),
            temp_slope         = get_coef("tempC"),
            time_p_value       = get_pval("time_numeric"),
            temp_p_value       = get_pval("tempC"),
            time_significant   = get_sig("time_numeric"),
            temp_significant   = get_sig("tempC"),
            adj_r2             = summary_model$adj.r.squared %||% 0
          )
        }) |>
        ungroup() |>
        # ---- Apply filters for model quality and significance ----
        filter(
          adj_r2 >= 0.3,
          (time_significant | temp_significant)
        )
    })
  )

# ---- Step 2: Apply Drift Correction ----
df_corrected <- df_combined %>%
  ungroup() |>
  mutate(
    profiles = map2(profiles, df_model$drift_model, ~ {
      .x %>%
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark")) %>%
        left_join(
          .y %>% filter((time_significant | temp_significant) & adj_r2 >= 0.2),
          by = c("experiment", "light_dark", "channel", "analyte")
        ) %>%
        mutate(
          time_numeric = as.numeric(difftime(date_time, min(date_time), units = "secs")),
          corrected_mV = case_when(
            !is.na(time_slope) & time_significant & temp_significant ~ 
              mV - (intercept + time_slope * time_numeric + temp_slope * tempC),
            !is.na(time_slope) & time_significant ~ 
              mV - (intercept + time_slope * time_numeric),
            !is.na(temp_slope) & temp_significant ~ 
              mV - (intercept + temp_slope * tempC),
            TRUE ~ mV
          )
        ) %>%
        select(
          -time_slope, -temp_slope, -time_significant, -temp_significant,
          -time_p_value, -temp_p_value, -time_numeric, -intercept, -adj_r2
        )
    })
  )
```

## üîß Plot Drift-Corrected Profiles

Now that sensor drift has been modeled and corrected, we can visualize the **cleaned microprofiles** to verify that the correction procedure worked as intended.  

This diagnostic plot overlays the **drift-corrected signal (mV)** for each core, flume, and analyte.  
It allows for quick comparison between experiments and helps confirm that baseline alignment has improved following correction.

```{r correction_plot, message=FALSE, warning=FALSE}
# ==== 1. Generate drift-corrected profile plots ====
df_corrected <- df_corrected |>
  mutate(
    profile_plot = map2(
      profiles, experiment,
      \(prof_df, exp_id) {
        ggplot(
          prof_df,
          aes(x = corrected_mV, y = depth, color = core)
        ) +
          geom_path(aes(linetype = channel), linewidth = 0.6, alpha = 0.8) +
          geom_point(aes(shape = channel), size = 2) +
          scale_y_reverse(expand = c(0.01, 0.01)) +
          facet_wrap(
            ~ flume + light_dark + analyte,
            ncol = 4,
            scales = "free_x"
          ) +
          labs(
            title = paste("Drift-corrected profiles:", exp_id),
            x = expression(paste("Corrected signal (mV)")),
            y = "Depth (mm)",
            color = "Core ID",
            linetype = "Channel",
            shape = "Channel"
          ) +
          theme_bw(base_size = 12) +
          theme(
            plot.title = element_text(size = 13, face = "bold", hjust = 0.5),
            panel.grid.minor = element_blank(),
            legend.position = "bottom",
            legend.box = "horizontal"
          )
      }
    )
  )

df_corrected |>
  filter(experiment == "Mangrove_8") |>
  pull(profile_plot)
```


# ‚öóÔ∏è Sensor Calibration

Once the sensor drift has been corrected, the next step is to **convert voltage (mV) readings into analyte concentrations (¬µmol L‚Åª¬π)** using the experimentally derived calibration curves.  

Calibration ensures that each sensor‚Äôs electrical response is properly scaled, allowing direct comparison across treatments, analytes, and experiments.

## üßÆ Applying Calibration Coefficients

For each experiment, we query the calibration table to identify whether **multiple calibrations** were performed during the same run.  

- If **slope and intercept values remain stable** (‚â§10% change), we apply the *first calibration* (pre-run).  
- If **drift between calibrations** is evident (>10% difference in slope), we apply a *time-weighted average* slope to approximate the mean calibration response. 

**üîç Purpose:**

- Transform corrected sensor voltages into **real concentration units**  
- Account for **slight shifts in sensor calibration curves** during measurement  
- Maintain **consistency** between sensors and across experiments  

```{r, calibration, message=FALSE, warning=FALSE}
df_calibrated <- df_corrected |>
  select(-profile_plot) |>
  mutate(
    profiles = map2(profiles, cals, \(prof_df, cal_df) {
      # ---- Normalize column names ----
      prof_df <- prof_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark"))
      cal_df <- cal_df |>
        rename_with(~ str_replace_all(., "light[\\.]*dark", "light_dark"))
      
      # ---- Harmonize key column types ----
      prof_df <- prof_df |>
        mutate(
          channel = as.character(channel),
          analyte = trimws(as.character(analyte)),
          light_dark = trimws(as.character(light_dark))
        )
      cal_df <- cal_df |>
        mutate(
          channel = as.character(channel),
          analyte = trimws(as.character(analyte)),
          light_dark = trimws(as.character(light_dark))
        )
      
      # ---- Pre-clean calibrations ----
      cal_df <- cal_df |>
        filter(!is.na(slope), !is.na(intercept)) |>
        group_by(experiment, light_dark, channel, analyte) |>
        group_modify(\(cal_group, key) {
          # If only one calibration ‚Üí use it
          if (nrow(cal_group) == 1) {
            return(mutate(cal_group, cal_selection = "pre_run"))
          }
          
          # Compute relative slope change (%)
          slope_diff <- (max(cal_group$slope, na.rm = TRUE) -
                           min(cal_group$slope, na.rm = TRUE)) /
            mean(cal_group$slope, na.rm = TRUE) * 100
          
          if (slope_diff <= 10) {
            # Slopes nearly identical ‚Üí use pre-run (first chronologically)
            cal_group |>
              arrange(cal_time) |>
              slice(1) |>
              mutate(cal_selection = "pre_run")
          } else {
            # ---- Robust time-weighted average ----
            slope_ordered <- cal_group |> arrange(cal_time)
            
            # Calculate safe weights (avoid division by zero)
            time_diffs <- as.numeric(difftime(
              max(slope_ordered$cal_time),
              slope_ordered$cal_time,
              units = "secs"
            ))
            time_diffs[time_diffs == 0] <- 1  # avoid Inf
            weights <- 1 / time_diffs
            weights <- weights / sum(weights, na.rm = TRUE)  # normalize
            
            slope_weighted <- weighted.mean(slope_ordered$slope,
                                            w = weights, na.rm = TRUE)
            intercept_mean <- mean(slope_ordered$intercept, na.rm = TRUE)
            
            slope_ordered |>
              slice_tail(n = 1) |>
              mutate(
                slope = slope_weighted,
                intercept = intercept_mean,
                cal_selection = "weighted_mean"
              )
          }
        }) |>
        ungroup()
      
      # ---- Apply selected calibration ----
      prof_df |>
        left_join(
          cal_df,
          by = c("experiment", "light_dark", "analyte", "channel")
        ) |>
        mutate(
          final_concentration = (corrected_mV - intercept) / slope
        )
    })
  )
```

üìä Inspect Calibrated Profiles

After applying the calibrations, we can visualize the converted profiles to confirm that each analyte‚Äôs concentration gradient behaves as expected.

```{r calibrated_profiles_plot, message=FALSE, warning=FALSE}
# ---- Plot function ----
df_plots <- df_calibrated |>
  mutate(
    profile_plot = map2(profiles, experiment, \(prof_df, exp_name) {
      ggplot(prof_df, aes(x = final_concentration, y = depth, color = core)) +
        geom_path(aes(linetype = channel), linewidth = 0.7) +
        geom_point(aes(shape = channel), size = 1.8, stroke = 0.5) +
        scale_y_reverse() +
        scale_color_brewer(palette = "Set2", name = "Core") +
        labs(
          title = exp_name,
          x = "Final concentration (¬µmol L‚Åª¬π)",
          y = "Depth (mm)"
        ) +
        facet_wrap(~ flume + light_dark + analyte, ncol = 4, scales = "free_x") +
        theme_bw(base_size = 11) +
        theme(
          panel.grid = element_blank(),
          legend.position = "bottom",
          legend.box = "horizontal",
          strip.text = element_text(size = 9, face = "bold")
        )
    })
  )

# ---- Plot profiles ----
df_plots |>
  filter(experiment == "Mangrove_8") |>
  pull(profile_plot)
```

# üéØ Final Profile Adjustments

After calibration, each microprofile is adjusted so that the **baseline (bottom or top)** aligns with zero.  
For most analytes (O‚ÇÇ, N‚ÇÇO, NO), we assume concentrations at the *bottom of the profile* are close to zero.  
For **H‚ÇÇS**, this assumption is reversed ‚Äî we expect near-zero concentrations at the *top of the profile* due to oxidation near the sediment‚Äìwater interface.

These minor baseline adjustments help standardize the datasets across replicates and ensure consistent profile comparisons.

## ‚öôÔ∏è Adjustment Procedure

For each analyte, we calculate the **mean of the bottom (or top) three points** and subtract this value from the entire profile.  
This yields an *adjusted concentration* where the baseline is set to zero while maintaining the integrity of the original gradient.

```{r profile_adjust, message=FALSE, warning=FALSE}
df_adjusted <- df_calibrated %>%
  mutate(
    profiles = map(profiles, ~ {
      .x %>%
        dplyr::group_by(light_dark, flume, core, replicate, channel, analyte) %>%
        arrange(if_else(analyte == "H2S", depth, desc(depth))) %>%  # Sort depths based on analyte
        dplyr::mutate(
          mean_bottom_three = mean(head(final_concentration, 3)),  # Take the top 3 rows after sorting
          adjusted_value = final_concentration - mean_bottom_three  # Subtract the mean from concentration1
        ) %>%
        ungroup()
    })
  )
```

üìä Plot Adjusted Profiles

After the baseline adjustment, we visualize the final, standardized concentration profiles to confirm that all analytes now land near zero at their expected ends of the profile domain.

```{r adjusted_profiles_plot, message=FALSE, warning=FALSE}
df_adj_plots <- df_adjusted |>
  mutate(
    profile_plot = map2(profiles, experiment, \(prof_df, exp_name) {
      ggplot(prof_df, aes(x = adjusted_value, y = depth, color = core)) +
        geom_path(aes(linetype = channel), linewidth = 0.7) +
        geom_point(aes(shape = channel), size = 1.8, stroke = 0.5) +
        scale_y_reverse() +
        scale_color_brewer(palette = "Set2", name = "Core") +
        labs(
          title = exp_name,
          x = "Adjusted concentration (¬µmol L‚Åª¬π)",
          y = "Depth (mm)"
        ) +
        facet_wrap(~ flume + light_dark + analyte, ncol = 4, scales = "free_x") +
        theme_bw(base_size = 11) +
        theme(
          panel.grid = element_blank(),
          legend.position = "bottom",
          legend.box = "horizontal",
          strip.text = element_text(size = 9, face = "bold")
        )
    })
  )

# Inspect each experiment
df_adj_plots %>%
  filter(experiment == "Mangrove_8") %>%
  pull(profile_plot)
```

üß™ Interpretation Guide:

- ‚úÖ Profiles land near zero at expected boundary ‚Üí baseline successfully standardized

- ‚ö†Ô∏è Offsets remain (>5 ¬µmol L‚Åª¬π) ‚Üí recheck calibration or replicate assignment

- üåÄ H‚ÇÇS shows inversion ‚Üí confirm top-down orientation of depth sorting

# üíæ Extract & Save Profiles

Finally, we export the clean, adjusted profiles for downstream statistical and visualization analyses.
You can either save the full dataset or separate analytes into individual .csv files for easier integration into external workflows.

```{r profile_write, message=FALSE, warning=FALSE}
## Extract & Save Final Adjusted Profiles
# Create output directory if it doesn‚Äôt exist
if (!dir.exists("./corrected-profiles")) dir.create("./corrected-profiles")

# Define helper function for export
export_profiles <- function(df, analyte_name, file_label) {
  df |>
    mutate(profiles = map(profiles, ~ select(.x, -experiment))) |>
    unnest(profiles) |>
    filter(analyte == analyte_name) |>
    select(
      experiment, date, light_dark, treatment, flume, core, channel, analyte,
      date_time, depth, mV, corrected_mV, final_concentration, adjusted_value
    ) |>
    write.csv(
      file = paste0("./corrected-profiles/MBGC.", file_label, ".profiles.corrected.csv"),
      row.names = FALSE
    )
}

# ---- Export each analyte ----
export_profiles(df_adjusted, "O2",   "oxygen")
export_profiles(df_adjusted, "N2O",  "nitrous")
export_profiles(df_adjusted, "NO",   "nitric")
export_profiles(df_adjusted, "H2S",  "sulfide")

message("‚úÖ All analyte-specific corrected profiles have been exported to './corrected-profiles/'")
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
